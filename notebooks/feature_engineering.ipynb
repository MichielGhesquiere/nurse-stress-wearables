{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a99940f",
   "metadata": {},
   "source": [
    "# Feature Engineering for Nurse Stress Detection\n",
    "\n",
    "This notebook focuses on creating meaningful features from physiological sensor data for stress detection classification.\n",
    "\n",
    "## Objectives:\n",
    "- Create time-based features\n",
    "- Generate rolling statistical features\n",
    "- Extract movement and activity features\n",
    "- Prepare data for machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f05753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE ENGINEERING FOR STRESS DETECTION ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=== FEATURE ENGINEERING FOR STRESS DETECTION ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d47a013b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1964182, 9)\n",
      "Date range: 2020-07-16 14:49:00 to 2020-12-13 08:01:00\n",
      "Subjects: [83 '6D' '83']\n",
      "Stress events: 1297937 (66.08%)\n"
     ]
    }
   ],
   "source": [
    "# Load the subset data\n",
    "df = pd.read_csv(r'C:\\Users\\Michi\\nurse-stress-wearables\\data\\merged_data_subset.csv')\n",
    "\n",
    "# Convert datetime and set as index for time series operations\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.sort_values(['id', 'datetime']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "print(f\"Subjects: {df['id'].unique()}\")\n",
    "print(f\"Stress events: {(df['label'] != 0).sum()} ({(df['label'] != 0).mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae81ce2",
   "metadata": {},
   "source": [
    "## 1. Basic Time-Based Features\n",
    "\n",
    "Extract temporal patterns that might influence stress levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f825162e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-based features created:\n",
      "   hour  day_of_week  is_weekend  is_night_shift  is_work_hours  hour_sin  \\\n",
      "0     6            4           0               1              0       1.0   \n",
      "1     6            4           0               1              0       1.0   \n",
      "2     6            4           0               1              0       1.0   \n",
      "3     6            4           0               1              0       1.0   \n",
      "4     6            4           0               1              0       1.0   \n",
      "\n",
      "       hour_cos   dow_sin   dow_cos  \n",
      "0  6.123234e-17 -0.433884 -0.900969  \n",
      "1  6.123234e-17 -0.433884 -0.900969  \n",
      "2  6.123234e-17 -0.433884 -0.900969  \n",
      "3  6.123234e-17 -0.433884 -0.900969  \n",
      "4  6.123234e-17 -0.433884 -0.900969  \n"
     ]
    }
   ],
   "source": [
    "def create_time_features(df):\n",
    "    \"\"\"Create time-based features from datetime.\"\"\"\n",
    "    df_time = df.copy()\n",
    "    \n",
    "    # Basic time features\n",
    "    df_time['hour'] = df_time['datetime'].dt.hour\n",
    "    df_time['day_of_week'] = df_time['datetime'].dt.dayofweek\n",
    "    df_time['is_weekend'] = df_time['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df_time['is_night_shift'] = ((df_time['hour'] >= 22) | (df_time['hour'] <= 6)).astype(int)\n",
    "    df_time['is_work_hours'] = ((df_time['hour'] >= 8) & (df_time['hour'] <= 18)).astype(int)\n",
    "    \n",
    "    # Cyclical encoding for hour (24-hour cycle)\n",
    "    df_time['hour_sin'] = np.sin(2 * np.pi * df_time['hour'] / 24)\n",
    "    df_time['hour_cos'] = np.cos(2 * np.pi * df_time['hour'] / 24)\n",
    "    \n",
    "    # Cyclical encoding for day of week (7-day cycle)\n",
    "    df_time['dow_sin'] = np.sin(2 * np.pi * df_time['day_of_week'] / 7)\n",
    "    df_time['dow_cos'] = np.cos(2 * np.pi * df_time['day_of_week'] / 7)\n",
    "    \n",
    "    return df_time\n",
    "\n",
    "# Apply time features\n",
    "df_features = create_time_features(df)\n",
    "\n",
    "# Display new time features\n",
    "time_cols = ['hour', 'day_of_week', 'is_weekend', 'is_night_shift', 'is_work_hours', \n",
    "             'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos']\n",
    "print(\"Time-based features created:\")\n",
    "print(df_features[time_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d27819",
   "metadata": {},
   "source": [
    "## 2. Rolling Window Statistical Features\n",
    "\n",
    "Create features that capture recent physiological trends and variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c55f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating rolling features (5 minutes)...\n",
      "Creating rolling features (10 minutes)...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 435. MiB for an array with shape (29, 1964182) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m df_features \u001b[38;5;241m=\u001b[39m create_rolling_features(df_features, window_minutes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating rolling features (10 minutes)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m df_features \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_rolling_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_minutes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Show sample of new rolling features\u001b[39;00m\n\u001b[0;32m     40\u001b[0m rolling_cols \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df_features\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col)][:\u001b[38;5;241m6\u001b[39m]\n",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m, in \u001b[0;36mcreate_rolling_features\u001b[1;34m(df, window_minutes)\u001b[0m\n\u001b[0;32m      5\u001b[0m window_size \u001b[38;5;241m=\u001b[39m window_minutes \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m\n\u001b[0;32m      7\u001b[0m physiological_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEDA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEMP\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m df_rolling \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m physiological_cols:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Rolling statistics\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     df_rolling[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mean_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_minutes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_rolling\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)[col]\u001b[38;5;241m.\u001b[39mrolling(\n\u001b[0;32m     13\u001b[0m         window\u001b[38;5;241m=\u001b[39mwindow_size, min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index(\u001b[38;5;241m0\u001b[39m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Michi\\anaconda3\\envs\\sensor\\lib\\site-packages\\pandas\\core\\generic.py:6830\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6681\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   6682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, deep: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   6683\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6684\u001b[0m \u001b[38;5;124;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[0;32m   6685\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6828\u001b[0m \u001b[38;5;124;03m    dtype: int64\u001b[39;00m\n\u001b[0;32m   6829\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6830\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6831\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   6832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(data, axes\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   6833\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6834\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Michi\\anaconda3\\envs\\sensor\\lib\\site-packages\\pandas\\core\\internals\\managers.py:604\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    601\u001b[0m         res\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 604\u001b[0m     \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\Michi\\anaconda3\\envs\\sensor\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1783\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1785\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1788\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1789\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Michi\\anaconda3\\envs\\sensor\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2267\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2269\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   2271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2272\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\Michi\\anaconda3\\envs\\sensor\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2301\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2298\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m bvals2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_concat_same_type(bvals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2300\u001b[0m argsort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(new_mgr_locs)\n\u001b[1;32m-> 2301\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2302\u001b[0m new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2304\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 435. MiB for an array with shape (29, 1964182) and data type float64"
     ]
    }
   ],
   "source": [
    "def create_rolling_features(df, window_minutes=10):\n",
    "    \"\"\"Create rolling statistical features for physiological signals.\"\"\"\n",
    "    \n",
    "    # Assuming 1Hz sampling rate (adjust if different)\n",
    "    window_size = window_minutes * 60\n",
    "    \n",
    "    physiological_cols = ['EDA', 'HR', 'TEMP']\n",
    "    df_rolling = df.copy()\n",
    "    \n",
    "    for col in physiological_cols:\n",
    "        # Rolling statistics\n",
    "        df_rolling[f'{col}_mean_{window_minutes}min'] = df_rolling.groupby('id')[col].rolling(\n",
    "            window=window_size, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        \n",
    "        df_rolling[f'{col}_std_{window_minutes}min'] = df_rolling.groupby('id')[col].rolling(\n",
    "            window=window_size, min_periods=1).std().reset_index(0, drop=True)\n",
    "        \n",
    "        df_rolling[f'{col}_min_{window_minutes}min'] = df_rolling.groupby('id')[col].rolling(\n",
    "            window=window_size, min_periods=1).min().reset_index(0, drop=True)\n",
    "        \n",
    "        df_rolling[f'{col}_max_{window_minutes}min'] = df_rolling.groupby('id')[col].rolling(\n",
    "            window=window_size, min_periods=1).max().reset_index(0, drop=True)\n",
    "        \n",
    "        # Rate of change features\n",
    "        df_rolling[f'{col}_slope'] = df_rolling.groupby('id')[col].diff()\n",
    "        \n",
    "        df_rolling[f'{col}_slope_mean_{window_minutes}min'] = df_rolling.groupby('id')[f'{col}_slope'].rolling(\n",
    "            window=window_size, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    return df_rolling\n",
    "\n",
    "# Apply rolling features with 5-minute and 10-minute windows\n",
    "print(\"Creating rolling features (5 minutes)...\")\n",
    "df_features = create_rolling_features(df_features, window_minutes=5)\n",
    "\n",
    "print(\"Creating rolling features (10 minutes)...\")\n",
    "df_features = create_rolling_features(df_features, window_minutes=10)\n",
    "\n",
    "# Show sample of new rolling features\n",
    "rolling_cols = [col for col in df_features.columns if 'min' in col and ('mean' in col or 'std' in col)][:6]\n",
    "print(f\"\\nSample rolling features:\")\n",
    "print(df_features[rolling_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d7bd1",
   "metadata": {},
   "source": [
    "## 3. Movement and Activity Features\n",
    "\n",
    "Extract features from accelerometer data to capture physical activity patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b87ac94a",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 15.0 MiB for an array with shape (1, 1964182) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_movement\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Apply movement features\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m df_features \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_movement_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Display movement features\u001b[39;00m\n\u001b[0;32m     43\u001b[0m movement_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovement_magnitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovement_std\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity_numeric\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity_mean_5min\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_acceleration\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m, in \u001b[0;36mcreate_movement_features\u001b[1;34m(df, window_minutes)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create movement and activity features from accelerometer data.\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m window_size \u001b[38;5;241m=\u001b[39m window_minutes \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m\n\u001b[1;32m----> 5\u001b[0m df_movement \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Movement magnitude\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df_movement[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovement_magnitude\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(df_movement[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m df_movement[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m df_movement[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Michi\\anaconda3\\envs\\sensor\\lib\\site-packages\\pandas\\core\\generic.py:6830\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6681\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   6682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, deep: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   6683\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6684\u001b[0m \u001b[38;5;124;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[0;32m   6685\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6828\u001b[0m \u001b[38;5;124;03m    dtype: int64\u001b[39;00m\n\u001b[0;32m   6829\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6830\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6831\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   6832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(data, axes\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   6833\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6834\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Michi\\anaconda3\\envs\\sensor\\lib\\site-packages\\pandas\\core\\internals\\managers.py:593\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    591\u001b[0m         new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m--> 593\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcopy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m res\u001b[38;5;241m.\u001b[39maxes \u001b[38;5;241m=\u001b[39m new_axes\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Michi\\anaconda3\\envs\\sensor\\lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\Michi\\anaconda3\\envs\\sensor\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:822\u001b[0m, in \u001b[0;36mBlock.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    820\u001b[0m refs: BlockValuesRefs \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 822\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    823\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 15.0 MiB for an array with shape (1, 1964182) and data type float64"
     ]
    }
   ],
   "source": [
    "def create_movement_features(df, window_minutes=5):\n",
    "    \"\"\"Create movement and activity features from accelerometer data.\"\"\"\n",
    "    \n",
    "    window_size = window_minutes * 60\n",
    "    df_movement = df.copy()\n",
    "    \n",
    "    # Movement magnitude\n",
    "    df_movement['movement_magnitude'] = np.sqrt(df_movement['X']**2 + df_movement['Y']**2 + df_movement['Z']**2)\n",
    "    \n",
    "    # Movement variability\n",
    "    df_movement['movement_std'] = df_movement.groupby('id')['movement_magnitude'].rolling(\n",
    "        window=window_size, min_periods=1).std().reset_index(0, drop=True)\n",
    "    \n",
    "    # Activity level (based on movement magnitude)\n",
    "    df_movement['activity_level'] = pd.cut(df_movement['movement_magnitude'], \n",
    "                                         bins=[0, 1, 2, 5, np.inf], \n",
    "                                         labels=['sedentary', 'light', 'moderate', 'vigorous'])\n",
    "    \n",
    "    # Convert activity level to numeric\n",
    "    activity_mapping = {'sedentary': 0, 'light': 1, 'moderate': 2, 'vigorous': 3}\n",
    "    df_movement['activity_numeric'] = df_movement['activity_level'].map(activity_mapping)\n",
    "    \n",
    "    # Rolling activity features\n",
    "    df_movement['activity_mean_5min'] = df_movement.groupby('id')['activity_numeric'].rolling(\n",
    "        window=window_size, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    # Movement direction changes (acceleration changes)\n",
    "    for axis in ['X', 'Y', 'Z']:\n",
    "        df_movement[f'{axis}_acceleration'] = df_movement.groupby('id')[axis].diff()\n",
    "    \n",
    "    df_movement['total_acceleration'] = np.sqrt(\n",
    "        df_movement['X_acceleration']**2 + \n",
    "        df_movement['Y_acceleration']**2 + \n",
    "        df_movement['Z_acceleration']**2\n",
    "    )\n",
    "    \n",
    "    return df_movement\n",
    "\n",
    "# Apply movement features\n",
    "df_features = create_movement_features(df_features)\n",
    "\n",
    "# Display movement features\n",
    "movement_cols = ['movement_magnitude', 'movement_std', 'activity_numeric', 'activity_mean_5min', 'total_acceleration']\n",
    "print(\"Movement features created:\")\n",
    "print(df_features[movement_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322fb306",
   "metadata": {},
   "source": [
    "## 4. Physiological Feature Engineering\n",
    "\n",
    "Create advanced features specific to stress detection from physiological signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f93f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_physiological_features(df):\n",
    "    \"\"\"Create advanced physiological features for stress detection.\"\"\"\n",
    "    \n",
    "    df_physio = df.copy()\n",
    "    \n",
    "    # Heart Rate Variability (simple approximation)\n",
    "    df_physio['HR_variability'] = df_physio.groupby('id')['HR'].rolling(\n",
    "        window=300, min_periods=1).std().reset_index(0, drop=True)\n",
    "    \n",
    "    # EDA features (stress-specific)\n",
    "    # EDA peaks (simple peak detection)\n",
    "    df_physio['EDA_diff'] = df_physio.groupby('id')['EDA'].diff()\n",
    "    df_physio['EDA_peaks'] = ((df_physio['EDA_diff'] > 0) & \n",
    "                             (df_physio['EDA_diff'].shift(-1) < 0)).astype(int)\n",
    "    \n",
    "    # EDA response magnitude\n",
    "    df_physio['EDA_response'] = df_physio.groupby('id')['EDA'].rolling(\n",
    "        window=60, min_periods=1).max().reset_index(0, drop=True) - \\\n",
    "                               df_physio.groupby('id')['EDA'].rolling(\n",
    "        window=60, min_periods=1).min().reset_index(0, drop=True)\n",
    "    \n",
    "    # Temperature stability\n",
    "    df_physio['TEMP_stability'] = 1 / (1 + df_physio.groupby('id')['TEMP'].rolling(\n",
    "        window=300, min_periods=1).std().reset_index(0, drop=True))\n",
    "    \n",
    "    # Physiological stress indicators\n",
    "    # Elevated HR (above personal baseline)\n",
    "    hr_baseline = df_physio.groupby('id')['HR'].transform('median')\n",
    "    df_physio['HR_elevated'] = (df_physio['HR'] > hr_baseline * 1.1).astype(int)\n",
    "    \n",
    "    # EDA elevation (above personal baseline)\n",
    "    eda_baseline = df_physio.groupby('id')['EDA'].transform('median')\n",
    "    df_physio['EDA_elevated'] = (df_physio['EDA'] > eda_baseline * 1.2).astype(int)\n",
    "    \n",
    "    return df_physio\n",
    "\n",
    "# Apply physiological features\n",
    "df_features = create_physiological_features(df_features)\n",
    "\n",
    "# Display physiological features\n",
    "physio_cols = ['HR_variability', 'EDA_peaks', 'EDA_response', 'TEMP_stability', 'HR_elevated', 'EDA_elevated']\n",
    "print(\"Physiological features created:\")\n",
    "print(df_features[physio_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771559a5",
   "metadata": {},
   "source": [
    "## 5. Feature Summary and Correlation Analysis\n",
    "\n",
    "Analyze the created features and their relationships with stress labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all feature columns (exclude original raw signals and metadata)\n",
    "exclude_cols = ['datetime', 'id', 'label', 'X', 'Y', 'Z', 'EDA', 'HR', 'TEMP', 'activity_level']\n",
    "feature_cols = [col for col in df_features.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Total features created: {len(feature_cols)}\")\n",
    "print(\"\\nFeature categories:\")\n",
    "\n",
    "# Categorize features\n",
    "time_features = [col for col in feature_cols if any(x in col for x in ['hour', 'day', 'weekend', 'night', 'work'])]\n",
    "rolling_features = [col for col in feature_cols if 'min' in col and any(x in col for x in ['mean', 'std', 'max', 'min'])]\n",
    "movement_features = [col for col in feature_cols if any(x in col for x in ['movement', 'activity', 'acceleration'])]\n",
    "physio_features = [col for col in feature_cols if any(x in col for x in ['variability', 'peaks', 'response', 'stability', 'elevated'])]\n",
    "slope_features = [col for col in feature_cols if 'slope' in col]\n",
    "\n",
    "print(f\"Time features: {len(time_features)}\")\n",
    "print(f\"Rolling statistical features: {len(rolling_features)}\")\n",
    "print(f\"Movement features: {len(movement_features)}\")\n",
    "print(f\"Physiological features: {len(physio_features)}\")\n",
    "print(f\"Slope features: {len(slope_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500667db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis with stress labels\n",
    "feature_correlations = df_features[feature_cols + ['label']].corr()['label'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 15 features correlated with stress:\")\n",
    "print(feature_correlations.head(16)[1:])  # Exclude self-correlation\n",
    "\n",
    "# Visualize top correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_correlations.head(16)[1:15]  # Top 15 excluding self\n",
    "plt.barh(range(len(top_features)), top_features.values)\n",
    "plt.yticks(range(len(top_features)), top_features.index)\n",
    "plt.xlabel('Absolute Correlation with Stress Label')\n",
    "plt.title('Top 15 Features Correlated with Stress')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f708ad",
   "metadata": {},
   "source": [
    "## 6. Handle Missing Values and Data Quality\n",
    "\n",
    "Clean the engineered features and prepare for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in engineered features\n",
    "missing_summary = df_features[feature_cols].isnull().sum()\n",
    "missing_features = missing_summary[missing_summary > 0]\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(\"Features with missing values:\")\n",
    "    print(missing_features)\n",
    "    \n",
    "    # Forward fill missing values (appropriate for time series)\n",
    "    for col in missing_features.index:\n",
    "        df_features[col] = df_features.groupby('id')[col].fillna(method='ffill')\n",
    "        df_features[col] = df_features.groupby('id')[col].fillna(method='bfill')\n",
    "else:\n",
    "    print(\"No missing values in engineered features!\")\n",
    "\n",
    "# Check for infinite values\n",
    "inf_summary = np.isinf(df_features[feature_cols]).sum()\n",
    "inf_features = inf_summary[inf_summary > 0]\n",
    "\n",
    "if len(inf_features) > 0:\n",
    "    print(f\"\\nFeatures with infinite values:\")\n",
    "    print(inf_features)\n",
    "    \n",
    "    # Replace infinite values\n",
    "    df_features = df_features.replace([np.inf, -np.inf], np.nan)\n",
    "    # Fill with median values\n",
    "    for col in inf_features.index:\n",
    "        median_val = df_features[col].median()\n",
    "        df_features[col] = df_features[col].fillna(median_val)\n",
    "else:\n",
    "    print(\"No infinite values found!\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df_features.shape}\")\n",
    "print(f\"Features ready for modeling: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b7e7a",
   "metadata": {},
   "source": [
    "## 7. Feature Visualization for Stress vs Normal Periods\n",
    "\n",
    "Compare feature distributions between stress and normal periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff33015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top features for visualization\n",
    "top_viz_features = feature_correlations.head(7)[1:7].index.tolist()  # Top 6 features\n",
    "\n",
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(top_viz_features):\n",
    "    stress_data = df_features[df_features['label'] != 0][feature].dropna()\n",
    "    normal_data = df_features[df_features['label'] == 0][feature].dropna()\n",
    "    \n",
    "    # Sample if too large for visualization\n",
    "    if len(normal_data) > 5000:\n",
    "        normal_data = normal_data.sample(5000)\n",
    "    \n",
    "    axes[i].hist(normal_data, bins=50, alpha=0.7, label='Normal', density=True, color='green')\n",
    "    if len(stress_data) > 0:\n",
    "        axes[i].hist(stress_data, bins=30, alpha=0.7, label='Stress', density=True, color='red')\n",
    "    \n",
    "    axes[i].set_title(f'{feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82754307",
   "metadata": {},
   "source": [
    "## 8. Save Engineered Features\n",
    "\n",
    "Save the processed dataset with all engineered features for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9668ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final feature set for modeling\n",
    "final_features = feature_cols + ['id', 'datetime', 'label']\n",
    "df_final = df_features[final_features].copy()\n",
    "\n",
    "# Save the engineered dataset\n",
    "output_path = r'C:\\Users\\Michi\\nurse-stress-wearables\\data\\engineered_features.csv'\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Engineered features saved to: {output_path}\")\n",
    "print(f\"Final dataset info:\")\n",
    "print(f\"- Shape: {df_final.shape}\")\n",
    "print(f\"- Features: {len(feature_cols)}\")\n",
    "print(f\"- Stress events: {(df_final['label'] != 0).sum()}\")\n",
    "print(f\"- Date range: {df_final['datetime'].min()} to {df_final['datetime'].max()}\")\n",
    "\n",
    "# Display feature summary\n",
    "print(f\"\\nFeature summary by category:\")\n",
    "print(f\"Time features ({len(time_features)}): {time_features[:3]}...\")\n",
    "print(f\"Rolling features ({len(rolling_features)}): {rolling_features[:3]}...\")\n",
    "print(f\"Movement features ({len(movement_features)}): {movement_features[:3]}...\")\n",
    "print(f\"Physiological features ({len(physio_features)}): {physio_features}\")\n",
    "\n",
    "print(\"\\n=== FEATURE ENGINEERING COMPLETE ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
